import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import joblib

# =======================
# 1Ô∏è‚É£ Load dataset
# =======================
df = pd.read_csv("Carbon Emission.csv")

print("üîπ Dataset Loaded Successfully")
print(df.head(), "\n")
print(df.info())

# =======================
# 2Ô∏è‚É£ Define features and target
# =======================
X = df.drop(columns=['CarbonEmission'])
y = df['CarbonEmission']

# =======================
# 3Ô∏è‚É£ Identify categorical & numeric columns
# =======================
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numeric_cols = X.select_dtypes(exclude=['object']).columns.tolist()

print("\nCategorical Columns:", categorical_cols)
print("Numeric Columns:", numeric_cols)

# =======================
# 4Ô∏è‚É£ Preprocessing
# =======================
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough'  # Keep numeric columns
)

# =======================
# 5Ô∏è‚É£ Build Pipeline
# =======================
model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(
        n_estimators=150,
        random_state=42,
        max_depth=15,
        min_samples_split=4,
        n_jobs=-1
    ))
])

# =======================
# 6Ô∏è‚É£ Train-Test Split
# =======================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# =======================
# 7Ô∏è‚É£ Train the Model
# =======================
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("\n‚úÖ Model trained successfully!")

# =======================
# 8Ô∏è‚É£ Evaluation Metrics
# =======================
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("\nüìä Model Evaluation Metrics:")
print(f"R¬≤ Score: {r2:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

# =======================
# 9Ô∏è‚É£ Visualization
# =======================

# --- 1. Actual vs Predicted Plot ---
plt.figure(figsize=(7, 6))
sns.scatterplot(x=y_test, y=y_pred, color='green', alpha=0.7)
plt.xlabel("Actual Emission")
plt.ylabel("Predicted Emission")
plt.title("Actual vs Predicted Carbon Emission")
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.show()

# --- 2. Residual Plot ---
residuals = y_test - y_pred
plt.figure(figsize=(7, 5))
sns.histplot(residuals, kde=True, color='purple')
plt.title("Distribution of Residuals (Error)")
plt.xlabel("Residuals")
plt.show()

# --- 3. Feature Importance ---
rf_model = model.named_steps['regressor']
feature_names = model.named_steps['preprocessor'].get_feature_names_out()
importance = pd.Series(rf_model.feature_importances_, index=feature_names)
top_features = importance.sort_values(ascending=False).head(15)

plt.figure(figsize=(10, 6))
sns.barplot(x=top_features, y=top_features.index, palette="viridis")
plt.title("Top 15 Important Features Affecting Carbon Emission")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

# =======================
# üîü Save Trained Model
# =======================
joblib.dump(model, "carbon_model.pkl")
print("\nüíæ Model saved as carbon_model.pkl")
